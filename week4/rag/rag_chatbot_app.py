"""Tr·ª£ l√Ω RAG v·ªõi kh·∫£ nƒÉng m·ªü r·ªông truy v·∫•n."""
import hashlib
import logging
import os
import re
import shutil
from typing import Any, Dict, List, Tuple

import chromadb
import streamlit as st
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain_chroma import Chroma
from langchain_community.document_loaders import (
    CSVLoader,
    DirectoryLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain_community.document_loaders.merge import MergedDataLoader
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv(override=True)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DATA_DIR = os.getenv("DATA_DIR", "data")
CHROMA_DB_PATH = os.path.join(DATA_DIR, "chroma_db")

# Thi·∫øt l·∫≠p ghi log
logging.basicConfig(
    level=logging.INFO,
    filename="rag_system.log",
    filemode="a",
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def display_message(message: str, message_type: str = "error") -> None:
    """Hi·ªÉn th·ªã th√¥ng b√°o tr√™n giao di·ªán Streamlit v·ªõi m√†u s·∫Øc theo lo·∫°i.

    Args:
        message: N·ªôi dung th√¥ng b√°o.
        message_type: Lo·∫°i th√¥ng b√°o ("error", "warning", "info").
    """
    if "message_placeholder" not in st.session_state:
        st.session_state.message_placeholder = st.empty()
    if "last_message" not in st.session_state or st.session_state.last_message != message:
        st.session_state.last_message = message
        message_id = f"message_{hashlib.md5(message.encode()).hexdigest()}"
        color = {
            "error": "#ff4b4b",
            "warning": "#ffcc00",
            "info": "#28a745",
        }.get(message_type, "#ff4b4b")
        html_message = f"""
        <div id="{message_id}" style="padding: 10px; margin-bottom: 10px; border-radius: 5px; 
        color: white; background-color: {color};">
            {message}
        </div>
        <script>
            setTimeout(function() {{
                var elem = document.getElementById("{message_id}");
                if (elem) {{
                    elem.parentNode.removeChild(elem);
                }}
            }}, 5000);
        </script>
        """
        st.session_state.message_placeholder.markdown(html_message, unsafe_allow_html=True)


class EmbeddingModel:
    """M√¥ h√¨nh Embedding ƒë·ªÉ t·∫°o vector t·ª´ vƒÉn b·∫£n."""

    def __init__(self, model_type: str = "openai"):
        self.model_type = model_type
        if model_type == "openai":
            self.embedding_function = OpenAIEmbeddings(
                api_key=OPENAI_API_KEY, model="text-embedding-3-small"
            )
        elif model_type == "huggingface":
            self.embedding_function = HuggingFaceEmbeddings(
                model_name="bkai-foundation-models/vietnamese-bi-encoder"
            )


class ChromaDBManager:
    """Qu·∫£n l√Ω c∆° s·ªü d·ªØ li·ªáu Chroma ƒë·ªÉ l∆∞u tr·ªØ vector."""

    def __init__(self, persist_directory: str = CHROMA_DB_PATH, embedding_function=None):
        os.makedirs(persist_directory, exist_ok=True)
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.embedding_function = embedding_function or HuggingFaceEmbeddings(
            model_name="bkai-foundation-models/vietnamese-bi-encoder"
        )
        try:
            # T·∫°o ho·∫∑c l·∫•y b·ªô s∆∞u t·∫≠p
            self.collection = self.client.get_or_create_collection(name="rag_docs")
            doc_count = self.collection.count()
            logger.info(f"Kh·ªüi t·∫°o b·ªô s∆∞u t·∫≠p ChromaDB 'rag_docs' v·ªõi {doc_count} t√†i li·ªáu")
        except Exception as e:
            logger.error(f"L·ªói kh·ªüi t·∫°o ChromaDB: {str(e)}")
            raise

    def create_vector_store(self, documents: List[Document] = None) -> Chroma:
        """T·∫°o vector store t·ª´ t√†i li·ªáu ho·∫∑c t·∫£i t·ª´ b·ªô nh·ªõ.

        Args:
            documents: Danh s√°ch t√†i li·ªáu ƒë·ªÉ t·∫°o vector store (m·∫∑c ƒë·ªãnh: None).

        Returns:
            Chroma: ƒê·ªëi t∆∞·ª£ng vector store.
        """
        if documents:
            return Chroma.from_documents(
                documents=documents,
                embedding=self.embedding_function,
                client=self.client,
                collection_name="rag_docs",
            )
        return Chroma(
            client=self.client,
            collection_name="rag_docs",
            embedding_function=self.embedding_function,
        )


class LLMModel:
    """M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."""

    def __init__(self, model_type: str = "openai"):
        self.model_type = model_type
        if model_type == "openai":
            self.client = ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-4o-mini")
        else:
            self.client = ChatOllama(model="llama3.2", temperature=0.1)


class DocumentProcessor:
    """X·ª≠ l√Ω t√†i li·ªáu: t·∫£i v√† chia nh·ªè th√†nh c√°c ƒëo·∫°n."""

    def __init__(self, embedding_type):
        self.semantic_splitter = SemanticChunker(
            embeddings=embedding_type,
            buffer_size=1,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95,
            min_chunk_size=500,
            add_start_index=True,
        )

    def load_files(self, files_path: str) -> List[Document]:
        """T·∫£i t√†i li·ªáu t·ª´ m·ªôt th∆∞ m·ª•c.

        Args:
            files_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a t√†i li·ªáu.

        Returns:
            List[Document]: Danh s√°ch c√°c t√†i li·ªáu ƒë√£ t·∫£i.
        """
        try:
            loaders = []
            if not os.path.exists(files_path):
                st.error(f"Th∆∞ m·ª•c {files_path} kh√¥ng t·ªìn t·∫°i")
                return []
            for filename in os.listdir(files_path):
                file_path = os.path.join(files_path, filename)
                try:
                    if filename.endswith(".txt"):
                        loaders.append(TextLoader(file_path))
                    elif filename.endswith(".md"):
                        loaders.append(UnstructuredMarkdownLoader(file_path))
                    elif filename.endswith(".pdf"):
                        loaders.append(PyPDFLoader(file_path))
                    elif filename.endswith(".csv"):
                        loaders.append(CSVLoader(file_path))
                except Exception as e:
                    print(f"L·ªói khi t·∫£i {filename}: {e}")
            merged_loader = MergedDataLoader(loaders=loaders)
            documents = merged_loader.load()
            # st.success(f"ƒê√£ t·∫£i l√™n {len(documents)} trang t√†i li·ªáu")
            display_message(f"ƒê√£ t·∫£i l√™n {len(documents)} trang t√†i li·ªáu", message_type="info")
            return documents
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i t√†i li·ªáu: {str(e)}")
            return []

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """Chia nh·ªè t√†i li·ªáu th√†nh c√°c ƒëo·∫°n.

        Args:
            documents: Danh s√°ch t√†i li·ªáu c·∫ßn chia nh·ªè.

        Returns:
            List[Document]: Danh s√°ch c√°c ƒëo·∫°n t√†i li·ªáu.
        """
        try:
            chunks = self.semantic_splitter.split_documents(documents)
            display_message(f"ƒê√£ chia th√†nh {len(chunks)} ƒëo·∫°n", message_type="info")
            return chunks
        except Exception as e:
            st.error(f"L·ªói khi chia t√†i li·ªáu: {str(e)}")
            return documents


class QueryExpansionRAG:
    """H·ªá th·ªëng RAG v·ªõi kh·∫£ nƒÉng m·ªü r·ªông truy v·∫•n."""

    def __init__(self, documents=None, embeddings=None, chroma_manager=None):
        self.embeddings = embeddings or HuggingFaceEmbeddings(
            model_name="bkai-foundation-models/vietnamese-bi-encoder"
        )
        if chroma_manager:
            self.chroma_manager = chroma_manager
            if documents:
                vector_store = self.chroma_manager.create_vector_store(documents)
            else:
                vector_store = self.chroma_manager.create_vector_store()
        else:
            vector_store = Chroma.from_documents(documents, self.embeddings)
        self.retriever = vector_store.as_retriever(
            search_kwargs={"k": 3}, search_type="similarity"
        )

    def retrieve_with_expansion(self, question: str) -> List[Document]:
        """T√¨m ki·∫øm t√†i li·ªáu li√™n quan ƒë·∫øn c√¢u h·ªèi.

        Args:
            question: C√¢u h·ªèi c·∫ßn t√¨m ki·∫øm.

        Returns:
            List[Document]: Danh s√°ch t√†i li·ªáu li√™n quan.
        """
        return self.retriever.invoke(question)


class AnswerGenerator:
    """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ nhi·ªÅu ngu·ªìn t√†i li·ªáu v·ªõi tr√≠ch d·∫´n."""

    def __init__(self, llm_type: str, temperature: float = 0):
        self.llm = LLMModel(llm_type)
        self.answer_generation_prompt = PromptTemplate(
            input_variables=["question"],
            template="""B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh, c√≥ nhi·ªám v·ª• cung c·∫•p c√¢u tr·∫£ l·ªùi chi ti·∫øt 
            d·ª±a tr√™n nhi·ªÅu ngu·ªìn t√†i li·ªáu. M·ª•c ti√™u l√† t·ªïng h·ª£p th√¥ng tin ch√≠nh x√°c v√† tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c.

            C√¢u h·ªèi: {question}
            
            D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n tr√≠ch t·ª´ c√°c t√†i li·ªáu, m·ªói ƒëo·∫°n c√≥ ID tr√≠ch d·∫´n:
            {formatted_context}

            Vui l√≤ng cung c·∫•p c√¢u tr·∫£ l·ªùi chi ti·∫øt:
            1. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi
            2. S·ª≠ d·ª•ng tr√≠ch d·∫´n c·ª• th·ªÉ theo ƒë·ªãnh d·∫°ng [CitationID]
            3. T·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn khi ph√π h·ª£p
            4. N√™u r√µ b·∫•t k·ª≥ m√¢u thu·∫´n n√†o gi·ªØa c√°c ngu·ªìn
            5. Tr√≠ch d·∫´n ƒëo·∫°n vƒÉn b·∫£n t·ª´ ngu·ªìn khi c·∫ßn thi·∫øt

            ƒê·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi:

            TR·∫¢ L·ªúI TR·ª∞C TI·∫æP:
            [C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn v·ªõi tr√≠ch d·∫´n]

            GI·∫¢I TH√çCH CHI TI·∫æT:
            [Gi·∫£i th√≠ch chi ti·∫øt v·ªõi tr√≠ch d·∫´n v√† ƒëo·∫°n tr√≠ch khi c·∫ßn]

            ƒêI·ªÇM CH√çNH:
            - [ƒêi·ªÉm 1 v·ªõi tr√≠ch d·∫´n]
            - [ƒêi·ªÉm 2 v·ªõi tr√≠ch d·∫´n]
            - [ƒêi·ªÉm 3 v·ªõi tr√≠ch d·∫´n]

            NGU·ªíN TR√çCH D·∫™N:
            [Li·ªát k√™ c√°c ID tr√≠ch d·∫´n v√† ƒë√≥ng g√≥p ch√≠nh]

            C√¢u tr·∫£ l·ªùi:""",
        )

    def _prepare_citation_chunks(
        self, documents: List[Document], max_chunk_length: int = 500
    ) -> Tuple[str, Dict[str, Dict[str, str]]]:
        """Chu·∫©n b·ªã ng·ªØ c·∫£nh v·ªõi tr√≠ch d·∫´n v√† t·∫°o b·∫£n ƒë·ªì tr√≠ch d·∫´n.

        Args:
            documents: Danh s√°ch t√†i li·ªáu li√™n quan.
            max_chunk_length: ƒê·ªô d√†i t·ªëi ƒëa cho m·ªói ƒëo·∫°n t√†i li·ªáu.

        Returns:
            Tuple[str, Dict]: Ng·ªØ c·∫£nh ƒë√£ ƒë·ªãnh d·∫°ng v√† b·∫£n ƒë·ªì tr√≠ch d·∫´n.
        """
        citation_id = 1
        citation_chunks = []
        citation_map = {}
        for doc in documents:
            content = doc.page_content
            truncated_content = content[:max_chunk_length]
            if len(content) > max_chunk_length:
                truncated_content += "..."
            citation_ref = f"[Citation{citation_id}]"
            citation_chunks.append(f"{citation_ref}:\n{truncated_content}\n")
            citation_map[citation_ref] = {"content": truncated_content, "full_content": content}
            citation_id += 1
        formatted_context = "\n".join(citation_chunks)
        return formatted_context, citation_map

    def generate_answer(self, question: str, documents: List[Document]) -> Dict[str, Any]:
        """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm v·ªõi tr√≠ch d·∫´n.

        Args:
            question: C√¢u h·ªèi g·ªëc.
            documents: Danh s√°ch t√†i li·ªáu li√™n quan.

        Returns:
            Dict[str, Any]: C√¢u tr·∫£ l·ªùi v√† th√¥ng tin tr√≠ch d·∫´n.
        """
        try:
            formatted_context, citation_map = self._prepare_citation_chunks(documents)
            response = self.llm.client.invoke(
                self.answer_generation_prompt.format(
                    question=question, formatted_context=formatted_context
                )
            )
            return {
                "answer": response.content,
                "citations": citation_map,
                "formatted_context": formatted_context,
            }
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}")
            return {
                "answer": "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi do l·ªói.",
                "citations": {},
                "formatted_context": "",
            }


def main() -> None:
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Streamlit."""
    st.set_page_config(
        page_title="Tr·ª£ l√Ω RAG",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.title("Tr·ª£ l√Ω RAG")

    # Kh·ªüi t·∫°o ƒë∆∞·ªùng d·∫´n v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    current_dir = os.path.dirname(os.path.abspath(__file__))
    files_directory = os.path.join(current_dir, "documents")

    # Kh·ªüi t·∫°o tr·∫°ng th√°i phi√™n
    if "rag_system" not in st.session_state:
        st.session_state.rag_system = None
    if "chroma_manager" not in st.session_state:
        st.session_state.chroma_manager = None
    if "doc_count" not in st.session_state:
        st.session_state.doc_count = 0

    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(files_directory, exist_ok=True)

    # Hi·ªÉn th·ªã th√¥ng tin h·ªá th·ªëng
    st.sidebar.title("Th√¥ng tin H·ªá th·ªëng")

    # Ch·ªçn m√¥ h√¨nh
    llm_type = st.sidebar.radio(
        "Ch·ªçn M√¥ h√¨nh LLM:",
        ["openai", "ollama"],
        format_func=lambda x: "OpenAI GPT-4" if x == "openai" else "Ollama Llama2",
    )
    embedding_type = st.sidebar.radio(
        "Ch·ªçn M√¥ h√¨nh Embedding:",
        ["openai", "huggingface"],
        format_func=lambda x: {
            "openai": "OpenAI Embeddings",
            "huggingface": "vietnamese-bi-encoder",
        }[x],
    )

    # ƒêi·ªÅu khi·ªÉn sidebar
    st.sidebar.title("ƒêi·ªÅu khi·ªÉn")
    uploaded_files = st.sidebar.file_uploader(
        "T·∫£i l√™n t·ªáp", type=["pdf", "txt", "md", "csv"], accept_multiple_files=True
    )

    if uploaded_files:
        st.sidebar.success(f"ƒê√£ t·∫£i l√™n {len(uploaded_files)} t·ªáp")
        for uploaded_file in uploaded_files:
            with open(os.path.join(files_directory, uploaded_file.name), "wb") as f:
                f.write(uploaded_file.getvalue())

    # Kh·ªüi t·∫°o EmbeddingModel v√† ChromaDBManager
    embeddings = EmbeddingModel(embedding_type)
    if st.session_state.chroma_manager is None:
        try:
            st.session_state.chroma_manager = ChromaDBManager(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=embeddings.embedding_function,
            )
            st.session_state.doc_count = st.session_state.chroma_manager.collection.count()
            display_message("Kh·ªüi t·∫°o ChromaDB th√†nh c√¥ng!", message_type="info")
        except Exception as e:
            logger.error(f"L·ªói kh·ªüi t·∫°o ChromaDB: {str(e)}")
            display_message(f"L·ªói kh·ªüi t·∫°o ChromaDB: {str(e)}", message_type="error")
            return

    # X·ª≠ l√Ω t√†i li·ªáu khi nh·∫•n n√∫t
    if st.sidebar.button("X·ª≠ l√Ω T√†i li·ªáu"):
        with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
            try:
                doc_processor = DocumentProcessor(embeddings.embedding_function)
                documents = doc_processor.load_files(files_directory)
                if documents:
                    splits = doc_processor.split_documents(documents)
                    st.session_state.rag_system = QueryExpansionRAG(
                        documents=splits,
                        embeddings=embeddings.embedding_function,
                        chroma_manager=st.session_state.chroma_manager,
                    )
                    st.session_state.doc_count = (
                        st.session_state.chroma_manager.collection.count()
                    )
                    display_message("X·ª≠ l√Ω t√†i li·ªáu th√†nh c√¥ng!", message_type="info")
                else:
                    display_message("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu", message_type="warning")
            except Exception as e:
                logger.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
                display_message(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}", message_type="error")

    # Giao di·ªán truy v·∫•n ch√≠nh
    st.header("Giao di·ªán Truy v·∫•n")
    query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")
    k = st.slider("S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ", min_value=1, max_value=10, value=3)

    # N√∫t t√¨m ki·∫øm
    if st.button("T√¨m ki·∫øm"):
        if query:
            if st.session_state.rag_system is None:
                display_message("Vui l√≤ng x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc!", message_type="warning")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω truy v·∫•n..."):
                try:
                    st.session_state.rag_system.retriever.search_kwargs["k"] = k
                    answer_generator = AnswerGenerator(llm_type)
                    docs = st.session_state.rag_system.retrieve_with_expansion(query)
                    st.subheader("üìù Ph√¢n t√≠ch Chi ti·∫øt")
                    with st.spinner("ƒêang t·∫°o c√¢u tr·∫£ l·ªùi chi ti·∫øt..."):
                        response_data = answer_generator.generate_answer(query, docs)
                        st.markdown(response_data["answer"])
                        st.subheader("üìö Ngu·ªìn Tr√≠ch d·∫´n")
                        for citation_id, citation_data in response_data["citations"].items():
                            with st.expander(f"{citation_id} - Nh·∫•n ƒë·ªÉ xem ngu·ªìn"):
                                st.markdown("**ƒêo·∫°n tr√≠ch:**")
                                st.markdown(f"```\n{citation_data['content']}\n```")
                                if st.button(f"Xem N·ªôi dung ƒê·∫ßy ƒë·ªß cho {citation_id}"):
                                    st.markdown("**N·ªôi dung ƒê·∫ßy ƒë·ªß:**")
                                    st.markdown(f"```\n{citation_data['full_content']}\n```")
                    with st.expander("üîé Xem T·∫•t c·∫£ K·∫øt qu·∫£ T√¨m ki·∫øm"):
                        for i, doc in enumerate(docs, 1):
                            st.markdown(f"**Truy v·∫•n:** {query}")
                            st.markdown(f"*T√†i li·ªáu {i}:*")
                            st.markdown(f"```\n{doc.page_content[:500]}...\n```")
                            st.markdown("---")
                    st.subheader("üéØ C√¢u tr·∫£ l·ªùi Cu·ªëi c√πng")
                    with st.spinner("ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi cu·ªëi c√πng..."):
                        final_prompt = PromptTemplate(
                            input_variables=["question", "detailed_answer"],
                            template="""D·ª±a tr√™n ph√¢n t√≠ch chi ti·∫øt, t·∫°o c√¢u tr·∫£ l·ªùi r√µ r√†ng, ng·∫Øn g·ªçn 
                            cho c√¢u h·ªèi g·ªëc. T·∫≠p trung v√†o c√°c ƒëi·ªÉm quan tr·ªçng v√† ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c.

                            C√¢u h·ªèi G·ªëc: {question}

                            Ph√¢n t√≠ch Chi ti·∫øt:
                            {detailed_answer}

                            C√¢u tr·∫£ l·ªùi cu·ªëi c√πng ph·∫£i:
                            1. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi
                            2. T√≥m t·∫Øt c√°c ƒëi·ªÉm ch√≠nh
                            3. R√µ r√†ng v√† ng·∫Øn g·ªçn
                            4. Gi·ªØ c√°c tr√≠ch d·∫´n quan tr·ªçng

                            C√¢u tr·∫£ l·ªùi Cu·ªëi c√πng:""",
                        )
                        llm_model = LLMModel(llm_type)
                        final_response = llm_model.client.invoke(
                            final_prompt.format(
                                question=query, detailed_answer=response_data["answer"]
                            )
                        )
                        st.markdown("---")
                        st.markdown("### üí° T√≥m t·∫Øt")
                        st.markdown(
                            f"""
                            <div style='background-color: #f0f2f6; padding: 20px; 
                            border-radius: 10px;'>
                            {final_response.content}
                            </div>
                            """,
                            unsafe_allow_html=True,
                        )
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω: {e}")
                    display_message(f"L·ªói h·ªá th·ªëng: {str(e)}", message_type="error")
        else:
            display_message("Vui l√≤ng nh·∫≠p c√¢u h·ªèi", message_type="warning")


if __name__ == "__main__":
    main()